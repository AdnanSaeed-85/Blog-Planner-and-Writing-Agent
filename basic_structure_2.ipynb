{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d1ad089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from CONFIG import GROQ_MODEL\n",
    "from pydantic import Field, BaseModel\n",
    "from typing import Annotated, List, TypedDict, Literal, Optional\n",
    "from langgraph.types import Send\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import operator\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from datetime import date, datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cfe73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# llm = ChatGroq(model=GROQ_MODEL)\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3e8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str = Field(..., description=\"Title of the blog.\")\n",
    "    audience: str = Field(..., description=\"The intended audience for this blog.\")\n",
    "    tone: str = Field(..., description=\"The writing tone (e.g., practical, crisp).\")\n",
    "\n",
    "    # NEW: tells workers what genre this is (prevents drift)\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task] = Field(..., description=\"List of tasks to structure the blog.\")\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str = Field(..., description=\"add title\")\n",
    "    url: str = Field(..., description=\"url\")\n",
    "    published_at: Optional[str] = Field(None, description=\"prefer ISO 'YYYY-MM-DD'\")\n",
    "    snippet: Optional[str] = Field(None, description=\"entire content\")\n",
    "    source: Optional[str] = Field(None, description='source will be here')\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    reason: str\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "    max_results_per_query: int = Field(5, description=\"How many results to fetch per query (3–8).\")\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe140e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class state_class(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # NEW: recency control\n",
    "    as_of: str           # ISO date, e.g. \"2026-01-29\"\n",
    "    recency_days: int    # 7 for weekly news, 30 for hybrid, etc.\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "    final: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e790db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: state_class) -> dict:\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(content=(\n",
    "                    \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "                    \"Hard requirements:\\n\"\n",
    "                    \"- Create 5–7 sections (tasks) that fit a technical blog.\\n\"\n",
    "                    \"- Each section must include:\\n\"\n",
    "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "                    \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "                    \"  3) target word count (120–450)\\n\"\n",
    "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "                    \"Make it technical (not generic):\\n\"\n",
    "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "                    \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
    "                    \"trade-offs → testing/observability → conclusion.\\n\"\n",
    "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "                    \"  * edge cases / failure modes\\n\"\n",
    "                    \"  * performance/cost considerations\\n\"\n",
    "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
    "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "                    \"to build/compare/measure/verify.\\n\\n\"\n",
    "                    \"Ordering guidance:\\n\"\n",
    "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
    "                    \"- Build core concepts before advanced details.\\n\"\n",
    "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "                    \"Output must strictly match the Plan schema.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic is here:- {state['topic']}\")\n",
    "\t  ]\n",
    "    )\n",
    "\n",
    "    return {'plans': plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b2b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- For open_book weekly roundup, include queries that reflect the last 7 days constraint.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: state_class) -> dict:\n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\\nAs-of date: {state['as_of']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set default recency window based on mode\n",
    "    if decision.mode == \"open_book\":\n",
    "        recency_days = 7\n",
    "    elif decision.mode == \"hybrid\":\n",
    "        recency_days = 45\n",
    "    else:\n",
    "        recency_days = 3650\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "        \"recency_days\": recency_days,\n",
    "    }\n",
    "\n",
    "def route_next(state: state_class) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c7a9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily)\n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Uses TavilySearchResults if installed and TAVILY_API_KEY is set.\n",
    "    Returns list of dict with common fields. Note: published date is often missing.\n",
    "    \"\"\"\n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _iso_to_date(s: Optional[str]) -> Optional[date]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return date.fromisoformat(s[:10])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- Extract/normalize published_at as ISO (YYYY-MM-DD) if you can infer it from title/snippet.\n",
    "  If you can't infer a date reliably, set published_at=null (do NOT guess).\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: state_class) -> dict:\n",
    "    queries = (state.get(\"queries\", []) or [])[:10]\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"As-of date: {state['as_of']}\\n\"\n",
    "                    f\"Recency days: {state['recency_days']}\\n\\n\"\n",
    "                    f\"Raw results:\\n{raw_results}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "    evidence = list(dedup.values())\n",
    "\n",
    "    # HARD RECENCY FILTER for open_book weekly roundup:\n",
    "    # keep only items with a parseable ISO date and within the window.\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "    if mode == \"open_book\":\n",
    "        as_of = date.fromisoformat(state[\"as_of\"])\n",
    "        cutoff = as_of - timedelta(days=int(state[\"recency_days\"]))\n",
    "        fresh: List[EvidenceItem] = []\n",
    "        for e in evidence:\n",
    "            d = _iso_to_date(e.published_at)\n",
    "            if d and d >= cutoff:\n",
    "                fresh.append(e)\n",
    "        evidence = fresh\n",
    "\n",
    "    return {\"evidence\": evidence}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274d9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Flexibility:\n",
    "- Do NOT use a fixed taxonomy unless it naturally fits.\n",
    "- You may tag tasks (tags field), but tags are flexible.\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book (weekly news roundup):\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections (no scraping/RSS/how to fetch news) unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient fresh sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: state_class) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    # Force blog_kind for open_book\n",
    "    forced_kind = \"news_roundup\" if mode == \"open_book\" else None\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\"\n",
    "                    f\"As-of: {state['as_of']} (recency_days={state['recency_days']})\\n\"\n",
    "                    f\"{'Force blog_kind=news_roundup' if forced_kind else ''}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\\n\\n\"\n",
    "                    f\"Instruction: If mode=open_book, your plan must NOT drift into a tutorial.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ensure open_book forces the kind even if model forgets\n",
    "    if forced_kind:\n",
    "        plan.blog_kind = \"news_roundup\"\n",
    "\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: state_class):\n",
    "    assert state[\"plan\"] is not None\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"as_of\": state[\"as_of\"],\n",
    "                \"recency_days\": state[\"recency_days\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "084a15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard (prevents mid-blog topic drift):\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book (weekly news):\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true (hybrid sections):\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning (concepts, intuition) is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "    as_of = payload.get(\"as_of\")\n",
    "    recency_days = payload.get(\"recency_days\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    # Provide a compact evidence list for citation use\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\"\n",
    "                    f\"As-of: {as_of} (recency_days={recency_days})\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    # deterministic ordering\n",
    "    return {\"sections\": [(task.id, section_md)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93ffe16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Reducer (merge + save)\n",
    "# -----------------------------\n",
    "def reducer_node(state: state_class) -> dict:\n",
    "    plan = state[\"plan\"]\n",
    "    if plan is None:\n",
    "        raise ValueError(\"Reducer called without a plan.\")\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    final_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb14a1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000013D83CE9850>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build graph\n",
    "# -----------------------------\n",
    "g = StateGraph(state_class)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_node)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "524681d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str, as_of: Optional[str] = None):\n",
    "    if as_of is None:\n",
    "        as_of = date.today().isoformat()\n",
    "\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"as_of\": as_of,\n",
    "            \"recency_days\": 7,   # router may overwrite\n",
    "            \"sections\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plan: Plan = out[\"plan\"]\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"TOPIC:\", topic)\n",
    "    print(\"AS_OF:\", out.get(\"as_of\"), \"RECENCY_DAYS:\", out.get(\"recency_days\"))\n",
    "    print(\"MODE:\", out.get(\"mode\"))\n",
    "    print(\"BLOG_KIND:\", plan.blog_kind)\n",
    "    print(\"NEEDS_RESEARCH:\", out.get(\"needs_research\"))\n",
    "    print(\"QUERIES:\", (out.get(\"queries\") or [])[:6])\n",
    "    print(\"EVIDENCE_COUNT:\", len(out.get(\"evidence\", [])))\n",
    "    if out.get(\"evidence\"):\n",
    "        print(\"EVIDENCE_SAMPLE:\", [e.model_dump() for e in out[\"evidence\"][:2]])\n",
    "    print(\"TASKS:\", len(plan.tasks))\n",
    "    print(\"SAVED_MD_CHARS:\", len(out.get(\"final\", \"\")))\n",
    "    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8948f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TOPIC: evaluation of LLMs till now\n",
      "AS_OF: 2026-02-06 RECENCY_DAYS: 45\n",
      "MODE: hybrid\n",
      "BLOG_KIND: news_roundup\n",
      "NEEDS_RESEARCH: True\n",
      "QUERIES: ['latest benchmarks for LLM performance in 2026', 'top LLMs in 2026 and their evaluation criteria', 'recent advancements in LLM capabilities and weaknesses', 'comparative analysis of leading LLMs as of 2026', 'notable case studies of LLM applications in 2026', 'user feedback trends on LLMs developed in the last year']\n",
      "EVIDENCE_COUNT: 13\n",
      "EVIDENCE_SAMPLE: [{'title': '8 LLM evaluation tools you should know in 2026', 'url': 'https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/', 'published_at': '2026-02-03', 'snippet': 'This guide spotlights eight pivotal LLM evaluation solutions, providing insights on how they support model assessment across various phases.', 'source': 'Tech HQ'}, {'title': 'LLM Comparison 2026: GPT-4 vs Claude vs Gemini and ...', 'url': 'https://www.ideas2it.com/blogs/llm-comparison', 'published_at': None, 'snippet': 'This blog provides a comprehensive comparison of leading LLMs, offering insights on performance and benchmarking against standard NLP datasets.', 'source': 'Ideas2IT'}]\n",
      "TASKS: 6\n",
      "SAVED_MD_CHARS: 13842\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'evaluation of LLMs till now',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['latest benchmarks for LLM performance in 2026',\n",
       "  'top LLMs in 2026 and their evaluation criteria',\n",
       "  'recent advancements in LLM capabilities and weaknesses',\n",
       "  'comparative analysis of leading LLMs as of 2026',\n",
       "  'notable case studies of LLM applications in 2026',\n",
       "  'user feedback trends on LLMs developed in the last year',\n",
       "  'latest research papers evaluating LLM technologies in 2026'],\n",
       " 'evidence': [EvidenceItem(title='8 LLM evaluation tools you should know in 2026', url='https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/', published_at='2026-02-03', snippet='This guide spotlights eight pivotal LLM evaluation solutions, providing insights on how they support model assessment across various phases.', source='Tech HQ'),\n",
       "  EvidenceItem(title='LLM Comparison 2026: GPT-4 vs Claude vs Gemini and ...', url='https://www.ideas2it.com/blogs/llm-comparison', published_at=None, snippet='This blog provides a comprehensive comparison of leading LLMs, offering insights on performance and benchmarking against standard NLP datasets.', source='Ideas2IT'),\n",
       "  EvidenceItem(title='Top 5 LLM Model Evaluation Platforms To Use In 2026', url='https://www.prompts.ai/blog/llm-model-evaluation-platforms-2026', published_at=None, snippet='DeepEval offers more than 14 targeted metrics for both RAG and fine-tuning, designed to align with recent advancements in LLM evaluation.', source='Prompts AI'),\n",
       "  EvidenceItem(title=\"Large Language Model Evaluation in '26: 10+ Metrics & ...\", url='https://research.aimultiple.com/large-language-model-evaluation/', published_at=None, snippet='We analyze key benchmarking methods for evaluating LLMs, emphasizing the need for diverse metrics to avoid overfitting.', source='AIMultiple'),\n",
       "  EvidenceItem(title='Large Language Models: What You Need to Know in 2026', url='https://hatchworks.com/blog/gen-ai/large-language-models-guide/', published_at=None, snippet='This post explores advancements in LLMs regarding translation efficiency, hallucinations, biases, and operational scalability.', source='Hatchworks'),\n",
       "  EvidenceItem(title='The Complete Guide to LLM Evaluation Tools in 2026', url='https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation', published_at='2026-01-09', snippet='This guide discusses structured evaluation frameworks necessary for ensuring the reliability and accuracy of LLM deployments.', source='Future AGI'),\n",
       "  EvidenceItem(title='Top open LLM for consumers, start of 2026, bookmark this ...', url='https://www.reddit.com/r/LocalLLaMA/comments/1q5lf5p/top_open_llm_for_consumers_start_of_2026_bookmark/', published_at=None, snippet='Discussion on open LLMs for consumers at the beginning of 2026. (Note: This post may not be fully available as it is deleted.)', source='Reddit'),\n",
       "  EvidenceItem(title='Top 10 Most Popular LLMs in 2026', url='https://zenmux.ai/blog/top-10-most-popular-llms-in-2026', published_at=None, snippet='This article reviews the most popular LLMs, including their strengths and use cases in 2026.', source='Zenmux'),\n",
       "  EvidenceItem(title='The best LLM evaluation tools of 2026 | by Dave Davies', url='https://medium.com/online-inference/the-best-llm-evaluation-tools-of-2026-40fd9b654dce', published_at='2026-01-06', snippet='Analyzes top LLM evaluation tools in 2026, emphasizing metrics for measuring performance, safety, and traceability.', source='Medium'),\n",
       "  EvidenceItem(title='Best Large Language Models 2026', url='https://xcelore.com/blog/best-ai-language-models-best-large-language-models-2026/', published_at=None, snippet='Comparison of AI language models in 2026, looking at performance and deployment options for enterprises.', source='Xcelore'),\n",
       "  EvidenceItem(title='7 LLM use cases and applications in 2026', url='https://www.assemblyai.com/blog/llm-use-cases', published_at=None, snippet='Shares insights on how various industries are implementing LLMs for tasks like document review, customer service, and more.', source='AssemblyAI'),\n",
       "  EvidenceItem(title='AI Leaderboards 2026 - Compare and rank the best AI models', url='https://llm-stats.com/', published_at=None, snippet='A platform for comparing and ranking LLMs across various benchmarks and metrics, providing insights for users.', source='LLM Stats'),\n",
       "  EvidenceItem(title='The trends that will shape AI and tech in 2026', url='https://www.ibm.com/think/news/ai-tech-trends-predictions-2026', published_at=None, snippet='Predictions on how LLMs will transition into dynamic agentic systems in 2026.', source='IBM')],\n",
       " 'plan': Plan(blog_title='Evaluating the Landscape of Large Language Models in 2026', audience='Developers and AI Practitioners', tone='practical, crisp', blog_kind='news_roundup', constraints=[], tasks=[Task(id=1, title='Overview of LLM Evaluation Metrics', goal='Understand the key metrics used for evaluating large language models.', bullets=['Define common metrics such as perplexity, BLEU, and accuracy for LLM evaluation.', 'Highlight the importance of context-specific metrics to assess model performance.', 'Discuss how diversity in metrics can prevent overfitting during evaluations.'], target_words=300, tags=['evaluation', 'metrics'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Emerging Evaluation Tools for LLMs in 2026', goal='Discover new tools and platforms for effectively evaluating LLM performance.', bullets=['Explore tools like DeepEval, focusing on over 14 targeted metrics for evaluation.', 'Review some evaluation frameworks that ensure reliability and accuracy in LLM assessments.', 'Analyze trends in the development of user-friendly evaluation interfaces.'], target_words=400, tags=['tools', 'frameworks'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Key Findings in Recent LLM Comparisons', goal='Summarize the latest comparative analyses of leading LLMs.', bullets=['Compare models such as GPT-4, Claude, and Gemini based on performance and efficiency metrics.', 'Identify standout features of each model highlighting their unique capabilities.', 'Assess the implications of these comparisons for choosing appropriate models for specific tasks.'], target_words=350, tags=['comparison', 'model_selection'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='LLM Improvements in Hallucination and Bias Management', goal='Examine advancements made in reducing biases and hallucinations in LLMs.', bullets=['Review current strategies deployed by leading models to minimize hallucinations during generation.', 'Discuss the impact of training data diversity on reducing biases in outputs.', 'Identify notable case studies or examples demonstrating effective bias management.'], target_words=300, tags=['bias', 'ethics'], requires_research=True, requires_citations=True, requires_code=False), Task(id=5, title='Insights on LLM Operational Scalability', goal='Understand factors affecting the scalability of LLMs in operational environments.', bullets=['Discuss the computational demands of deploying large models at scale and their implications.', 'Explore strategies like model pruning and quantization for improved efficiency.', 'Highlight examples of organizations successfully scaling LLM operations.'], target_words=350, tags=['scalability', 'performance'], requires_research=False, requires_citations=False, requires_code=False), Task(id=6, title='Future Directions in LLM Evaluation Methodologies', goal='Identify potential future trends in the evaluation of LLMs.', bullets=['Predict how evaluation methodologies may evolve to address increasing model complexities.', 'Discuss the role of user-generated feedback in changing evaluation approaches.', 'Speculate on the integration of AI-driven evaluation tools for real-time assessments.'], target_words=300, tags=['future_trends', 'evaluation'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': '2026-02-06',\n",
       " 'recency_days': 45,\n",
       " 'sections': [(1,\n",
       "   \"## Overview of LLM Evaluation Metrics\\n\\nEvaluating large language models (LLMs) requires a deep understanding of various metrics, each serving different purposes. Some common metrics include:\\n\\n- **Perplexity**: Measures how well a probability distribution predicts a sample. Lower perplexity indicates that the model is better at predicting the next word in a sequence.\\n- **BLEU (Bilingual Evaluation Understudy)**: Primarily used in translation tasks, BLEU evaluates the quality of generated text by comparing it to one or more reference translations. Higher scores signify better performance.\\n- **Accuracy**: A straightforward metric that calculates the proportion of correct predictions made by the model. It is commonly used in classification tasks.\\n\\nIn addition to these standard metrics, context-specific metrics are crucial for more nuanced evaluations. For instance, metrics such as F1 score or ROUGE may be more relevant for specific applications like summarization or question-answering. This flexibility allows developers to assess model performance based on the particular use case, enabling a more tailored evaluation process.\\n\\nMoreover, diversity in metrics is essential to prevent overfitting during evaluations. Relying solely on a single metric can lead to misleading conclusions about the model's capabilities. By employing a variety of metrics, practitioners can obtain a more comprehensive view of a model's performance, ensuring it performs well across multiple facets.\\n\\nIn summary, understanding these metrics and their context is vital for developers and AI practitioners working with LLMs. The appropriate selection of evaluation criteria not only aids in benchmarking but also enhances the overall utility and reliability of the models being developed and deployed. For a detailed exploration of LLM evaluation metrics, you can refer to sources such as [Large Language Model Evaluation in '26: 10+ Metrics & ...](https://research.aimultiple.com/large-language-model-evaluation/) and [The Complete Guide to LLM Evaluation Tools in 2026](https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation).\"),\n",
       "  (2,\n",
       "   '## Emerging Evaluation Tools for LLMs in 2026\\n\\nAs large language models (LLMs) continue to evolve, the necessity for robust evaluation tools has never been clearer. A range of emerging tools has been designed to refine the way we assess LLM performance, ensuring detailed insights and comparisons.\\n\\nOne prominent tool in this landscape is **DeepEval**, which focuses on more than 14 targeted metrics for evaluation. DeepEval provides a comprehensive approach, allowing developers to evaluate aspects such as accuracy, efficiency, and contextual understanding within LLMs. This metric-driven framework facilitates a nuanced assessment, ensuring that practitioners can make informed decisions based on diverse criteria. For further details on DeepEval, you can explore the full article [here](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/).\\n\\nIn addition to specific tools, various evaluation frameworks are gaining traction, emphasizing reliability and accuracy. These frameworks are structured to standardize evaluations across different models, diminishing biases arising from subjective assessments. With accurate benchmarks in place, developers are better equipped to compare performances among competing LLMs, fostering a clearer understanding of their strengths and weaknesses. Such advancements in frameworks are elaborated upon in the resources available [here](https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation).\\n\\nFinally, we are witnessing notable trends in the design and implementation of user-friendly evaluation interfaces. The emphasis on accessibility allows practitioners with varying levels of expertise to utilize these tools effectively. User experience innovations enhance usability, making intricate evaluation metrics more comprehensible and actionable. This trend is not only democratizing access to advanced evaluation features but also fostering a culture of data-driven decision-making. Insights on the latest trends in user interfaces can be explored further in this resource ([source](https://www.prompts.ai/blog/llm-model-evaluation-platforms-2026)).\\n\\nIn conclusion, the landscape of LLM evaluation tools is rapidly advancing, with solutions like DeepEval leading the way in metric-driven assessments. Comprehensive evaluation frameworks ensure reliability, while user-friendly interfaces promise accessibility for all practitioners. As these evaluations grow more sophisticated, they enable a more informed approach to selecting and implementing LLMs, crucial for achieving optimal performance in diverse applications.'),\n",
       "  (3,\n",
       "   '## Key Findings in Recent LLM Comparisons\\n\\nRecent analyses have offered insights into the performance and efficiency of leading large language models (LLMs) including GPT-4, Claude, and Gemini. Each of these models has distinct characteristics that influence their suitability for various tasks.\\n\\n### Performance and Efficiency Metrics\\n\\nThe comparative evaluations reveal that GPT-4 continues to excel in creative text generation and complex problem-solving, showcasing superior contextual understanding. Claude, noted for its reliability and safety features, performs exceptionally well in conversational tasks and ethical AI applications. Gemini stands out with its rapid inferencing and efficiency, making it an optimal choice for real-time applications requiring lower latency. These distinctions stem from their architectural choices and training methodologies, impacting their operational efficiency and resource consumption ([LLM Comparison 2026](https://www.ideas2it.com/blogs/llm-comparison)).\\n\\n### Standout Features\\n\\n- **GPT-4**: Best for nuanced creativity, particularly effective in producing compelling narratives and intricate analyses. It integrates advanced reasoning capabilities.\\n- **Claude**: Prioritizes user safety and ethical considerations, making it suitable for sensitive applications where conversational integrity is crucial.\\n- **Gemini**: Optimized for speed and efficiency, this model is a strong candidate for applications requiring quick response times, such as customer service chatbots and automated trading systems.\\n\\n### Implications for Model Selection\\n\\nThe findings emphasize the importance of aligning the choice of LLM with the specific requirements of your application. For tasks needing high creativity and depth, GPT-4 remains a strong candidate. Conversely, for scenarios that prioritize ethical interaction and user safety, Claude is recommended. Gemini, with its efficiency, appeals to developers looking to implement LLMs in high-frequency context environments. As these evaluations continue to evolve, leveraging comparative insights will be critical in selecting the right model to enhance functionality and user experience in AI-driven solutions ([8 LLM evaluation tools you should know in 2026](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/)).'),\n",
       "  (4,\n",
       "   '## LLM Improvements in Hallucination and Bias Management\\n\\nRecent advancements in large language models (LLMs) focus significantly on mitigating hallucinations and reducing biases in generated outputs. Current strategies deployed by leading models include enhanced validation mechanisms that cross-check generated facts against reliable datasets. These validation steps help ensure that the content produced by LLMs is more accurate and trustworthy, thereby reducing the instances of hallucination. Models are increasingly leveraging feedback loops from user interactions to refine their outputs continually, a method that allows systematic improvements and corrections over time ([Source](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/)).\\n\\nTraining data diversity plays a fundamental role in tackling bias within LLMs. Recent studies indicate that models trained on a more varied set of data exhibit reduced biases and provide fairer outputs. This improvement stems from the representation of diverse perspectives and backgrounds in training datasets, which helps to create more balanced models. LLMs that incorporate a wide range of voices are less likely to produce stereotypical or biased content ([Source](https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation)).\\n\\nA notable example of effective bias management is seen in the adjustments made by popular models like GPT-4. The developers have employed targeted retraining with diverse, ethically sourced datasets to counteract previous biases identified through user feedback. This case illustrates how incorporating user insights can directly enhance model reliability and ethical standards. These proactive measures reflect a growing awareness within the AI community about the importance of ethical AI development ([Source](https://www.prompts.ai/blog/llm-model-evaluation-platforms-2026)). \\n\\nIn conclusion, as LLMs continue to evolve, the integration of strategic data management and user-centric feedback loops will be essential to minimizing hallucinations and biases, ultimately leading to more reliable and ethical AI applications.'),\n",
       "  (5,\n",
       "   '## Insights on LLM Operational Scalability\\n\\nDeploying large language models (LLMs) at scale presents notable computational challenges that organizations must navigate to ensure efficiency and performance. The demands for memory, processing power, and storage can significantly escalate as LLMs increase in size. These factors frequently result in increased costs, longer deployment times, and the need for more sophisticated hardware. Consequently, developers must closely consider their infrastructure and resource management as they integrate LLMs into operational environments.\\n\\nTo mitigate these demands, various strategies have emerged, including model pruning and quantization. Model pruning involves systematically removing parts of the model that contribute little to its performance, effectively reducing the number of parameters and computational load without sacrificing accuracy. Quantization, on the other hand, reduces the precision of the model parameters with minimal impact on performance, thereby lowering memory usage and speeding up inference times. By embracing these techniques, organizations can enhance the efficiency of their LLM deployments, making it feasible to scale operations without a linear increase in resource investment.\\n\\nSuccessful examples of organizations scaling LLM operations can be observed in various sectors. For instance, companies have reported significant performance improvements while managing costs effectively through the adoption of optimized models. According to industry reports, organizations that utilized quantized LLM versions demonstrated noteworthy reductions in latency, enabling real-time applications in user-facing services. Furthermore, companies leveraging model pruning techniques have noted enhanced scalability, which allows them to deploy LLMs across multiple platforms and support a variety of applications simultaneously. \\n\\nAs the operational landscape of LLMs evolves, organizations must remain cognizant of the efficiency strategies at their disposal. The interplay between computational demands and optimization techniques will be crucial in shaping how LLMs are integrated into business operations. As discussed, the successes of early adopters serve as a blueprint for others looking to harness the potential of LLMs while maintaining scalable and cost-effective operations.'),\n",
       "  (6,\n",
       "   '## Future Directions in LLM Evaluation Methodologies\\n\\nAs large language models (LLMs) continue to evolve in complexity and capability, the methodologies used to evaluate these models are likely to undergo significant transformations. One key direction is the enhancement of evaluation frameworks to better capture the nuanced behavior of advanced models. This will involve moving beyond traditional metrics like accuracy and perplexity to incorporate more comprehensive assessments addressing real-world applicability, interpretability, and ethical considerations. For instance, automated benchmarking tools could be designed to evaluate not just the output but the thought process behind model decisions.\\n\\nUser-generated feedback is becoming an integral part of the evaluation landscape. As users interact with LLMs, their insights and critiques can inform and refine evaluation criteria. This shift towards participatory evaluation will likely lead to a collaborative ecosystem where feedback mechanisms are built into the model lifecycle. By harnessing user experiences, LLM developers can gain an iterative understanding of strengths and weaknesses, ultimately guiding improvements and enhancing user satisfaction.\\n\\nAdditionally, the integration of AI-driven evaluation tools for real-time assessments is on the horizon. Such tools could leverage natural language processing to analyze user interactions and model outputs in real-time, providing continuous performance feedback. This dynamic evaluation system could adaptively refine LLMs based on immediate user needs, leading to more responsive and relevant applications. Platforms like those mentioned in the recent articles on evaluation tools highlight emerging technologies that could facilitate this transformation ([Tech HQ](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/), [AIMultiple](https://research.aimultiple.com/large-language-model-evaluation/)). \\n\\nIn summary, the future of LLM evaluation methodologies promises to leverage evolving metrics and user engagement, augmented by real-time AI evaluations. These changes are expected to foster a more adaptive, user-centric approach to assessing the capabilities and impacts of LLMs in various domains.')],\n",
       " 'final': \"# Evaluating the Landscape of Large Language Models in 2026\\n\\n## Overview of LLM Evaluation Metrics\\n\\nEvaluating large language models (LLMs) requires a deep understanding of various metrics, each serving different purposes. Some common metrics include:\\n\\n- **Perplexity**: Measures how well a probability distribution predicts a sample. Lower perplexity indicates that the model is better at predicting the next word in a sequence.\\n- **BLEU (Bilingual Evaluation Understudy)**: Primarily used in translation tasks, BLEU evaluates the quality of generated text by comparing it to one or more reference translations. Higher scores signify better performance.\\n- **Accuracy**: A straightforward metric that calculates the proportion of correct predictions made by the model. It is commonly used in classification tasks.\\n\\nIn addition to these standard metrics, context-specific metrics are crucial for more nuanced evaluations. For instance, metrics such as F1 score or ROUGE may be more relevant for specific applications like summarization or question-answering. This flexibility allows developers to assess model performance based on the particular use case, enabling a more tailored evaluation process.\\n\\nMoreover, diversity in metrics is essential to prevent overfitting during evaluations. Relying solely on a single metric can lead to misleading conclusions about the model's capabilities. By employing a variety of metrics, practitioners can obtain a more comprehensive view of a model's performance, ensuring it performs well across multiple facets.\\n\\nIn summary, understanding these metrics and their context is vital for developers and AI practitioners working with LLMs. The appropriate selection of evaluation criteria not only aids in benchmarking but also enhances the overall utility and reliability of the models being developed and deployed. For a detailed exploration of LLM evaluation metrics, you can refer to sources such as [Large Language Model Evaluation in '26: 10+ Metrics & ...](https://research.aimultiple.com/large-language-model-evaluation/) and [The Complete Guide to LLM Evaluation Tools in 2026](https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation).\\n\\n## Emerging Evaluation Tools for LLMs in 2026\\n\\nAs large language models (LLMs) continue to evolve, the necessity for robust evaluation tools has never been clearer. A range of emerging tools has been designed to refine the way we assess LLM performance, ensuring detailed insights and comparisons.\\n\\nOne prominent tool in this landscape is **DeepEval**, which focuses on more than 14 targeted metrics for evaluation. DeepEval provides a comprehensive approach, allowing developers to evaluate aspects such as accuracy, efficiency, and contextual understanding within LLMs. This metric-driven framework facilitates a nuanced assessment, ensuring that practitioners can make informed decisions based on diverse criteria. For further details on DeepEval, you can explore the full article [here](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/).\\n\\nIn addition to specific tools, various evaluation frameworks are gaining traction, emphasizing reliability and accuracy. These frameworks are structured to standardize evaluations across different models, diminishing biases arising from subjective assessments. With accurate benchmarks in place, developers are better equipped to compare performances among competing LLMs, fostering a clearer understanding of their strengths and weaknesses. Such advancements in frameworks are elaborated upon in the resources available [here](https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation).\\n\\nFinally, we are witnessing notable trends in the design and implementation of user-friendly evaluation interfaces. The emphasis on accessibility allows practitioners with varying levels of expertise to utilize these tools effectively. User experience innovations enhance usability, making intricate evaluation metrics more comprehensible and actionable. This trend is not only democratizing access to advanced evaluation features but also fostering a culture of data-driven decision-making. Insights on the latest trends in user interfaces can be explored further in this resource ([source](https://www.prompts.ai/blog/llm-model-evaluation-platforms-2026)).\\n\\nIn conclusion, the landscape of LLM evaluation tools is rapidly advancing, with solutions like DeepEval leading the way in metric-driven assessments. Comprehensive evaluation frameworks ensure reliability, while user-friendly interfaces promise accessibility for all practitioners. As these evaluations grow more sophisticated, they enable a more informed approach to selecting and implementing LLMs, crucial for achieving optimal performance in diverse applications.\\n\\n## Key Findings in Recent LLM Comparisons\\n\\nRecent analyses have offered insights into the performance and efficiency of leading large language models (LLMs) including GPT-4, Claude, and Gemini. Each of these models has distinct characteristics that influence their suitability for various tasks.\\n\\n### Performance and Efficiency Metrics\\n\\nThe comparative evaluations reveal that GPT-4 continues to excel in creative text generation and complex problem-solving, showcasing superior contextual understanding. Claude, noted for its reliability and safety features, performs exceptionally well in conversational tasks and ethical AI applications. Gemini stands out with its rapid inferencing and efficiency, making it an optimal choice for real-time applications requiring lower latency. These distinctions stem from their architectural choices and training methodologies, impacting their operational efficiency and resource consumption ([LLM Comparison 2026](https://www.ideas2it.com/blogs/llm-comparison)).\\n\\n### Standout Features\\n\\n- **GPT-4**: Best for nuanced creativity, particularly effective in producing compelling narratives and intricate analyses. It integrates advanced reasoning capabilities.\\n- **Claude**: Prioritizes user safety and ethical considerations, making it suitable for sensitive applications where conversational integrity is crucial.\\n- **Gemini**: Optimized for speed and efficiency, this model is a strong candidate for applications requiring quick response times, such as customer service chatbots and automated trading systems.\\n\\n### Implications for Model Selection\\n\\nThe findings emphasize the importance of aligning the choice of LLM with the specific requirements of your application. For tasks needing high creativity and depth, GPT-4 remains a strong candidate. Conversely, for scenarios that prioritize ethical interaction and user safety, Claude is recommended. Gemini, with its efficiency, appeals to developers looking to implement LLMs in high-frequency context environments. As these evaluations continue to evolve, leveraging comparative insights will be critical in selecting the right model to enhance functionality and user experience in AI-driven solutions ([8 LLM evaluation tools you should know in 2026](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/)).\\n\\n## LLM Improvements in Hallucination and Bias Management\\n\\nRecent advancements in large language models (LLMs) focus significantly on mitigating hallucinations and reducing biases in generated outputs. Current strategies deployed by leading models include enhanced validation mechanisms that cross-check generated facts against reliable datasets. These validation steps help ensure that the content produced by LLMs is more accurate and trustworthy, thereby reducing the instances of hallucination. Models are increasingly leveraging feedback loops from user interactions to refine their outputs continually, a method that allows systematic improvements and corrections over time ([Source](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/)).\\n\\nTraining data diversity plays a fundamental role in tackling bias within LLMs. Recent studies indicate that models trained on a more varied set of data exhibit reduced biases and provide fairer outputs. This improvement stems from the representation of diverse perspectives and backgrounds in training datasets, which helps to create more balanced models. LLMs that incorporate a wide range of voices are less likely to produce stereotypical or biased content ([Source](https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation)).\\n\\nA notable example of effective bias management is seen in the adjustments made by popular models like GPT-4. The developers have employed targeted retraining with diverse, ethically sourced datasets to counteract previous biases identified through user feedback. This case illustrates how incorporating user insights can directly enhance model reliability and ethical standards. These proactive measures reflect a growing awareness within the AI community about the importance of ethical AI development ([Source](https://www.prompts.ai/blog/llm-model-evaluation-platforms-2026)). \\n\\nIn conclusion, as LLMs continue to evolve, the integration of strategic data management and user-centric feedback loops will be essential to minimizing hallucinations and biases, ultimately leading to more reliable and ethical AI applications.\\n\\n## Insights on LLM Operational Scalability\\n\\nDeploying large language models (LLMs) at scale presents notable computational challenges that organizations must navigate to ensure efficiency and performance. The demands for memory, processing power, and storage can significantly escalate as LLMs increase in size. These factors frequently result in increased costs, longer deployment times, and the need for more sophisticated hardware. Consequently, developers must closely consider their infrastructure and resource management as they integrate LLMs into operational environments.\\n\\nTo mitigate these demands, various strategies have emerged, including model pruning and quantization. Model pruning involves systematically removing parts of the model that contribute little to its performance, effectively reducing the number of parameters and computational load without sacrificing accuracy. Quantization, on the other hand, reduces the precision of the model parameters with minimal impact on performance, thereby lowering memory usage and speeding up inference times. By embracing these techniques, organizations can enhance the efficiency of their LLM deployments, making it feasible to scale operations without a linear increase in resource investment.\\n\\nSuccessful examples of organizations scaling LLM operations can be observed in various sectors. For instance, companies have reported significant performance improvements while managing costs effectively through the adoption of optimized models. According to industry reports, organizations that utilized quantized LLM versions demonstrated noteworthy reductions in latency, enabling real-time applications in user-facing services. Furthermore, companies leveraging model pruning techniques have noted enhanced scalability, which allows them to deploy LLMs across multiple platforms and support a variety of applications simultaneously. \\n\\nAs the operational landscape of LLMs evolves, organizations must remain cognizant of the efficiency strategies at their disposal. The interplay between computational demands and optimization techniques will be crucial in shaping how LLMs are integrated into business operations. As discussed, the successes of early adopters serve as a blueprint for others looking to harness the potential of LLMs while maintaining scalable and cost-effective operations.\\n\\n## Future Directions in LLM Evaluation Methodologies\\n\\nAs large language models (LLMs) continue to evolve in complexity and capability, the methodologies used to evaluate these models are likely to undergo significant transformations. One key direction is the enhancement of evaluation frameworks to better capture the nuanced behavior of advanced models. This will involve moving beyond traditional metrics like accuracy and perplexity to incorporate more comprehensive assessments addressing real-world applicability, interpretability, and ethical considerations. For instance, automated benchmarking tools could be designed to evaluate not just the output but the thought process behind model decisions.\\n\\nUser-generated feedback is becoming an integral part of the evaluation landscape. As users interact with LLMs, their insights and critiques can inform and refine evaluation criteria. This shift towards participatory evaluation will likely lead to a collaborative ecosystem where feedback mechanisms are built into the model lifecycle. By harnessing user experiences, LLM developers can gain an iterative understanding of strengths and weaknesses, ultimately guiding improvements and enhancing user satisfaction.\\n\\nAdditionally, the integration of AI-driven evaluation tools for real-time assessments is on the horizon. Such tools could leverage natural language processing to analyze user interactions and model outputs in real-time, providing continuous performance feedback. This dynamic evaluation system could adaptively refine LLMs based on immediate user needs, leading to more responsive and relevant applications. Platforms like those mentioned in the recent articles on evaluation tools highlight emerging technologies that could facilitate this transformation ([Tech HQ](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/), [AIMultiple](https://research.aimultiple.com/large-language-model-evaluation/)). \\n\\nIn summary, the future of LLM evaluation methodologies promises to leverage evolving metrics and user engagement, augmented by real-time AI evaluations. These changes are expected to foster a more adaptive, user-centric approach to assessing the capabilities and impacts of LLMs in various domains.\\n\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run('evaluation of LLMs till now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0effed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def generate_text(prompt):\n",
    "    response = client.models.generate_content_stream(model=\"gemini-2.5-flash\",contents=prompt)\n",
    "    for chunk in response:\n",
    "        if chunk.text:\n",
    "            for char in chunk.text:\n",
    "                sys.stdout.write(char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.005)\n",
    "                \n",
    "generate_text(\"write 5 lines of paragraph blog on AI Agnet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-writing-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
